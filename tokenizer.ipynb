{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"YLRqn3y2NfJi"},"source":["## Parallelized Tokenizer\n","---"]},{"cell_type":"markdown","source":["We find the most frequently appearing tokens in our large file. The function `top_n_tokens` returns a list of the n most frequently appearing tokens and their frequencies. This is the simple implementation in basic Python."],"metadata":{"id":"gOBkD-4FaqV-"}},{"cell_type":"code","source":["from collections import Counter\n","\n","def top_n_tokens(filename):\n","    # tokenizing file\n","    tokenized = []\n","    with open(filename) as f:\n","        for line in f:\n","            # tokenize, one line at a time\n","            '''\n","            REDACTED\n","            '''\n","\n","    c = Counter(tokenized) # create instance of Counter class\n","    top_n_tokens_list = c.most_common(n) # find the n most frequent tokens\n","                                                 #   and the number of occurrences\n","    return top_n_tokens_list"],"metadata":{"id":"NOo33X1ka597"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","The function `t_dict` returns a dictionary. The keys are tokens that follow the token `t` on more than one line. The values are the number of lines in which the pattern is observed. This is the simple implementation in basic Python."],"metadata":{"id":"lM4_INiQbMod"}},{"cell_type":"code","source":["def t_dict(filename, t):\n","    dict_t = {}\n","    term = None # a temp variable for holding the tokens\n","\n","    with open(filename) as f:\n","        for line in f:\n","            # tokenize, one line at a time\n","            t = simple_tokenize(line)\n","\n","            for i in range((len(t) - 1)): # checking all but the last token on a line\n","                '''\n","                REDACTED\n","                '''\n","\n","    dict_t = {k:v for k,v in dict_t.items() if v > 1}\n","      # keep only those key value pairs where the value (count) is greater than 1\n","    return dict_t"],"metadata":{"id":"PTic0kwtbwEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z3DCzT-rNfJm"},"source":["---\n","Analyzing the [pointwise mutual information (PMI)](http://en.wikipedia.org/wiki/Pointwise_mutual_information) of tokens.\n","\n","This chunk analyzes how large this problem will be. Determine the number of *distinct* tokens and distinct token pairs that exist."]},{"cell_type":"code","metadata":{"id":"AWP7tAfsNfJp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"57c4a481-8ddf-4849-bd36-2fd56185a214"},"source":["import itertools\n","\n","single_tokens = {}\n","token_pairs = {}\n","\n","with open('_.txt') as f:\n","    for line in f:\n","        # tokenize, one line at a time\n","        t = simple_tokenize(line)\n","\n","        unique_t = list(set(t)) # get just the unique tokens\n","        '''\n","        REDACTED\n","        '''\n","\n","        pairs = list(itertools.combinations(unique_t,2)) # find all pairs of tokens\n","\n","        pairs = [list(pairs[i]) for i in range(len(pairs))] # convert tuples to list\n","        pairs_copy = pairs[:] # make a shallow copy of the list of pairs\n","\n","        '''\n","        REDACTED\n","        '''\n","\n","        pairs = [tuple(pairs[i]) for i in range(len(pairs))] # convert pairs back to tuples to be hashable\n","\n","        for pair in pairs:\n","            if pair not in token_pairs: # check whether the token pair already exists in the dictionary\n","                token_pairs[pair] = 1 # if not, add it to the dictionary"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25975\n","1969760\n"]}]},{"cell_type":"markdown","metadata":{"id":"PpFi9CxkNfJq"},"source":["---\n","Implement the token queries."]},{"cell_type":"code","metadata":{"id":"3of7ltFENfJr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e481caa-a7b0-4f8e-eb24-28b1b1c91154"},"source":["# the log function for computing PMI\n","from math import log\n","import itertools\n","\n","######################################################################\n","\n","single_tokens = {}\n","token_pairs = {}\n","n_lines = 0\n","\n","with open('_.txt') as f:\n","    for line in f:\n","        n_lines = n_lines + 1\n","        # tokenize, one line at a time\n","        t = simple_tokenize(line)\n","        unique_t = list(set(t)) # get just the unique tokens\n","        '''\n","        REDACTED\n","        '''\n","\n","######################################################################\n","\n","while True:\n","    q = input(\"Input: \")\n","    if len(q) == 0:\n","        break\n","    '''\n","    REDACTED\n","    '''\n","    if len(q_tokens) == 1:\n","        threshold = 0\n","        '''\n","        REDACTED\n","        '''\n","        if q_tokens[0] in single_tokens: # checking that the token exists\n","            n_x = single_tokens[q_tokens[0]] # lookup the number of occurrences\n","\n","            tokens = list(single_tokens.keys()) # get all token keys in the dictionary\n","            valid_pairs = []\n","\n","            for token in tokens:\n","              pair = [q_tokens[0], token] # create a pair from input and each token in dictionary\n","              if (token != q_tokens[0]) and (tuple(pair) in token_pairs): # check that the token is not equal to\n","                                                                          #   input and that the pair exists\n","                n_xy = token_pairs[tuple(pair)] # lookup number of occurences of pair\n","                if n_xy >= threshold:\n","                    valid_pairs.append(pair) # store pair if it occurs at least threshold many times\n","\n","            top_5_pmis = []\n","\n","            for i in range(0, 5):\n","                max_pmi = 0\n","                max_pmi_key = None\n","                max_pmi_n_xy = 0\n","\n","                '''\n","                REDACTED\n","                '''\n","\n","                else:\n","                    break # break out of loop if there are fewer than 5 valid pairs\n","                top_5_pmis.append([max_pmi_key, max_pmi, max_pmi_n_xy]) # store the token pair, it's pmi and n(x,y)\n","\n","        else: # if the token doesn't exist\n","            n_x = 0 # set n(x) = 0\n","            top_5_pmis = [] # no pairs to consider\n","\n","        '''\n","        REDACTED\n","        '''\n","\n","\n","    elif len(q_tokens) == 2:\n","\n","        pair = [q_tokens[0], q_tokens[1]]\n","\n","        if tuple(pair) in token_pairs: # check that the pair exists\n","          '''\n","          REDACTED\n","          '''\n","\n","        else:  # if the pair doesn't exist\n","            n_xy = 0 # set n(x, y) = 0\n","            pmi = 0 # pmi(x, y) = 0\n","\n","        '''\n","        REDACTED\n","        '''\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Input 1 or 2 space-separated tokens (return to quit): the end\n","  n(the,end) = 157\n","  PMI(the,end) = 0.3505058356267105\n","Input 1 or 2 space-separated tokens (return to quit): end\n","Input a positive integer frequency threshold: 3\n","  n(end) = 353\n","  high PMI tokens with respect to end (threshold: 3):\n","    n(end,rope's) = 5,  PMI(end,rope's) = 2.5402124568157887\n","    n(end,upper) = 3,  PMI(end,upper) = 2.1722356715211943\n","    n(end,latter) = 7,  PMI(end,latter) = 2.1548615754517715\n","    n(end,begun) = 4,  PMI(end,begun) = 1.679874450244795\n","    n(end,world's) = 3,  PMI(end,world's) = 1.3183637071994323\n","Input 1 or 2 space-separated tokens (return to quit): \n"]}]},{"cell_type":"markdown","source":["---\n","We again count the number of *distinct* tokens but now use Spark to do so."],"metadata":{"id":"SbUiNMV8fu6P"}},{"cell_type":"code","source":["def distinct_tokens():\n","    lines = sc.textFile(\"_.txt\") # import file\n","    words = lines.flatMap(lambda x: simple_tokenize(x)).map(lambda word: (word, 1))\n","                                  # tokenize and create tuples with prelim count\n","    numdistinct = words.reduceByKey(lambda x, y: 1).values().reduce(lambda x, y: x + y)\n","                                  # extract 1s and take their sum\n","    return numdistinct\n"],"metadata":{"id":"dQGSos1nf3O_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","This Spark program counts the number of distinct token pairs as before."],"metadata":{"id":"usvzxwwQf6-b"}},{"cell_type":"code","source":["import itertools\n","\n","# Returns the count of distinct pairs\n","def distinct_pairs():\n","    lines = sc.textFile(\"_.txt\") # import file\n","    tokenized_lines = lines.map(lambda x: simple_tokenize(x)) # tokenize lines\n","    distinct_token_lines = tokenized_lines.map(lambda x: list(set(x))) # keep only\n","                                  # a distinct set of words\n","\n","    # Returns all combinations of two words in a sentence\n","    def pair(x):\n","        '''\n","        REDACTED\n","        '''\n","\n","    pairs = distinct_token_lines.flatMap(lambda x: pair(x)) # apply pair function and flatten\n","    pairs_reverse = pairs.map(lambda x: x[::-1]) # reversing all the pairs\n","    '''\n","    REDACTED\n","    '''\n","    numdistinct = pairs.reduceByKey(lambda x, y: 1).values().reduce(lambda x, y: x + y)\n","                                  # extract 1s and take their sum\n","\n","    return numdistinct\n"],"metadata":{"id":"gQc1Ce4AgDhn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","Here we calculate PMIs for every distinct token and n highest-probability tokens in a data-parallel fashion."],"metadata":{"id":"an4gIDbAgGpV"}},{"cell_type":"code","source":["# Returns a list of the top n (probability, count, token) tuples, ordered by probability\n","def top_n_tokens_prob(n):\n","    lines = sc.textFile(\"_.txt\") # import file\n","    tokenized_lines = lines.map(lambda x: simple_tokenize(x)).flatMap(lambda x: list(set(x)))\n","                                    # tokenize the lines, get unique tokens then flatten\n","\n","    counts = tokenized_lines.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n","                                    # create tuples then take the sum of 1s\n","    numLines = lines.count() # the number of lines in the file\n","\n","    probabilities = counts.mapValues(lambda x: x / numLines) # compute probability for each token\n","\n","    tokens = probabilities.join(counts) # join probability and count on token key\n","\n","    # Reorders the entries in x\n","    def reorder(x):\n","        '''\n","        REDACTED\n","        '''\n","\n","    result = tokens.map(lambda x: reorder(x)).sortByKey(ascending=False) # reorder the\n","                                      # entries in the RDD\n","\n","    # Collapses x into a list from a list of lists\n","    def collapse(x):\n","        '''\n","        REDACTED\n","        '''\n","\n","    result = result.map(lambda x: collapse(x)).take(n) # collapse all the entries in the RDD\n","    return result\n"],"metadata":{"id":"ZweCVUXEgV24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","PMIs done by using Spark."],"metadata":{"id":"evQcT9oBgYJr"}},{"cell_type":"code","source":["import itertools\n","from math import log\n","\n","# Returns a list of tuples with the following format:\n","'''\n","REDACTED\n","'''\n","def PMI(threshold):\n","    n_lines = sc.accumulator(0) # initialize accumulator for number of lines\n","    sc.textFile(\"_.txt\").foreach(lambda x: n_lines.add(1)) # increment accumulator\n","                                    # for each line\n","    lines = sc.textFile(\"_.txt\") # import file\n","    tokenized_lines = lines.map(lambda x: simple_tokenize(x)).map(lambda x: list(set(x)))\n","                                    # tokenize lines, keep only unique tokens\n","    words = lines.map(lambda x: simple_tokenize(x)).flatMap(lambda x: list(set(x)))\n","                                    # tokenize lines, keep unique tokens and flatten\n","\n","    counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n","                                    # create tuples then take the sum of 1s\n","    counts = dict(counts.collect()) # turn counts into a dictionary for easy lookup\n","    counts = sc.broadcast(counts) # broadcast the dictionary\n","\n","    # Returns all combinations of two words in a sentence\n","    def pair(x):\n","        '''\n","        REDACTED\n","        '''\n","\n","    n_lines = n_lines.value\n","\n","    pairs = tokenized_lines.flatMap(lambda x: pair(x)) # apply pair function and flatten\n","    pairs_reverse = pairs.map(lambda x: x[::-1]) # reversing all the pairs\n","    pairs = pairs.union(pairs_reverse).map(lambda x: tuple(x)).map(lambda word: (word, 1))\n","                                  # including the reversed pairs in the overall list\n","    coOccurrences = pairs.reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] >= threshold)\n","\n","    '''\n","    REDACTED\n","    '''\n","\n","    return result.collect()\n"],"metadata":{"id":"IWCUTo7Ugo87"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","Take a threshold $T$ and a sample size $N$ and find co-occuring tokens, as well as PMI$(x,y)$ and more. Choosse $N$ different $x$'s, uniformly at random to output."],"metadata":{"id":"BUZQMvyhguMn"}},{"cell_type":"code","source":["from math import log\n","\n","# Returns a list of samp_size tuples with the following format:\n","'''\n","REDACTED\n","'''\n","\n","def PMI_one_tok(threshold, samp_size):\n","    n_lines = sc.accumulator(0) # initialize accumulator for number of lines\n","    sc.textFile(\"_.txt\").foreach(lambda x: n_lines.add(1)) # increment accumulator\n","                                    # for each line\n","    lines = sc.textFile(\"_.txt\") # import file\n","    tokenized_lines = lines.map(lambda x: simple_tokenize(x)).map(lambda x: list(set(x)))\n","                                    # tokenize lines, keep only unique tokens\n","    words = lines.map(lambda x: simple_tokenize(x)).flatMap(lambda x: list(set(x)))\n","                                    # tokenize lines, keep unique tokens and flatten\n","\n","    counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n","                                    # create tuples then take the sum of 1s\n","    counts = dict(counts.collect()) # turn counts into a dictionary for easy lookup\n","    counts = sc.broadcast(counts) # broadcast the dictionary\n","\n","    # Returns all combinations of two words in a sentence\n","    def pair(x):\n","        '''\n","        REDACTED\n","        '''\n","\n","    n_lines = n_lines.value\n","\n","    pairs = tokenized_lines.flatMap(lambda x: pair(x)) # apply pair function and flatten\n","    pairs_reverse = pairs.map(lambda x: x[::-1]) # reversing all the pairs\n","    pairs = pairs.union(pairs_reverse).map(lambda x: tuple(x)).map(lambda word: (word, 1))\n","                                    # including the reversed pairs in the overall list\n","    coOccurrences = pairs.reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] >= threshold)\n","\n","    # Appends b to a\n","    def appender(a,b):\n","        '''\n","        REDACTED\n","        '''\n","\n","    # Extends a with b\n","    def extender(a, b):\n","        '''\n","        REDACTED\n","        '''\n","\n","    coOccurrences = coOccurrences.map(lambda x: (x[0][0], ((x[0][0], x[0][1]), x[1])))\n","                                    # this format makes the first token the key\n","                                    # so that it may be combined on\n","\n","    coOccurrences = coOccurrences.combineByKey(lambda x: [x],\n","                                               appender,\n","                                               extender).takeSample(False, samp_size)\n","                                    # combine the co-occuring tokens into one list\n","                                    # take a random sample of size N\n","\n","    coOccurrences = sc.parallelize(coOccurrences) # turn back into an RDD\n","\n","    # Creates the required format for the output\n","    def formatting(a):\n","        r = [] # initialize empty list\n","        for p in a:\n","          '''\n","          REDACTED\n","          '''\n","        return r\n","\n","    result = coOccurrences.map(lambda y: (y[0], formatting(y[1])))\n","                                  # apply the formatting function to all entries in RDD\n","\n","    return result.collect()\n"],"metadata":{"id":"Szd1qqJJg_bQ"},"execution_count":null,"outputs":[]}]}