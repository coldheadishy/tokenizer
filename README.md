# Parallelized Tokenizer

This project is a parallelized tokenizer on Pyspark. It involves doing some analysis of very very large plaintext files. I demonstrate it first in basic Python, then do the same task using MapReduce. Then I bask in how much faster the latter is.

This is a preview of the project, it is non-excutable in this form, just a few peaks at some code chunks. This is because it was a project for school and I need to comply with university policies. Of course, without there is also perhaps not enough context given here to fully grasp what is being done. I apologize for that, I hope that some of the documentation at least fills in a few of the details.

If you wish to see this project in full please contact me and state intent (ie. you are an employer/recruiter) and I can share a password protected repository that hosts the project in full.
